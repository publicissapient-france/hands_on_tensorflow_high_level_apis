{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cifar10 with tf.keras, tf.data and image augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-10T18:47:16.610755Z",
     "start_time": "2019-04-10T18:47:14.895672Z"
    }
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "import shutil\n",
    "import tensorflow as tf\n",
    "import tensorflow_hub as hub\n",
    "\n",
    "import numpy as np\n",
    "%matplotlib inline\n",
    "import random\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-10T18:47:16.614793Z",
     "start_time": "2019-04-10T18:47:16.612195Z"
    }
   },
   "outputs": [],
   "source": [
    "# Python version 3.5 or 3.6\n",
    "assert sys.version_info >= (3, 5)\n",
    "assert sys.version_info < (3, 7)\n",
    "# Tensorflow 2.0\n",
    "assert tf.__version__ >= \"2.0\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The problem we are trying to solve here is to classify RGB images (32 pixels by 32 pixels), into their 10 categories (_airplane_, _automobile_, _bird_, _cat_, _deer_, _dog_, _frog_, _horse_, _ship_, _truck_). The dataset we will use is the CIFAR10 dataset, a classic dataset in the machine learning community."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Input Data Management"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Download the dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The CIFAR10 dataset comes pre-loaded in Keras, in the form of a set of four Numpy arrays.\n",
    "\n",
    "Documentation : https://www.tensorflow.org/api_docs/python/tf/keras/datasets/cifar10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-10T18:47:17.139651Z",
     "start_time": "2019-04-10T18:47:16.827872Z"
    }
   },
   "outputs": [],
   "source": [
    "(train_images, train_labels), (test_images, test_labels) = tf.keras.datasets.cifar10.load_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualize the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's print the shapes of the datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-10T18:47:18.419878Z",
     "start_time": "2019-04-10T18:47:18.414732Z"
    }
   },
   "outputs": [],
   "source": [
    "print(\"Train images shape : {}\".format(train_images.shape))\n",
    "print(\"Train labels shape : {}\".format(train_labels.shape))\n",
    "print(\"Test images shape : {}\".format(test_images.shape))\n",
    "print(\"Test labels shape : {}\".format(test_labels.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see how the images look. This function shows a random example along with it's corresponding label."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-10T18:47:20.892786Z",
     "start_time": "2019-04-10T18:47:20.784322Z"
    }
   },
   "outputs": [],
   "source": [
    "i = random.randint(0, 100)\n",
    "\n",
    "print(\"Label: %s\" % train_labels[i])\n",
    "plt.imshow(train_images[i], cmap='gray')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Thats a little blurry !"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our workflow will be as follow: first we will present our neural network with the training data, `train_images` and `train_labels`. The network will then learn to associate images and labels. Finally, we will ask the network to produce predictions for `test_images`, and we will verify if these predictions match the labels from `test_labels`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create a tf.data Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will implement the input preprocessing function, composed of the following steps :\n",
    "- `cast` image to float32\n",
    "- `divide` image values by 255\n",
    "- `resize_image_with_crop_or_pad` to resize images to (224, 224, 3)\n",
    "- `one_hot` encoding of the label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-10T18:53:59.059017Z",
     "start_time": "2019-04-10T18:53:59.055894Z"
    }
   },
   "outputs": [],
   "source": [
    "NUM_CAT = 10\n",
    "\n",
    "def _preprocess_inputs(img, label):\n",
    "    # Cast image to float32\n",
    "    img_float = tf.cast(img, tf.float32)\n",
    "    # Divide image values by 255\n",
    "    img_norm = tf.divide(img_float, 255)\n",
    "    # Resize image to (224, 224, 3)\n",
    "    img_resize = tf.image.resize(img_norm, [224, 224])\n",
    "    \n",
    "    label_int = label[0]\n",
    "    # One Hot encoding of the label\n",
    "    label_one_hot = tf.one_hot(label_int, depth=NUM_CAT)\n",
    "    \n",
    "    return img_resize, label_one_hot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will apply image augmentation in the input processing pipeline. Here are the transformations we can apply :\n",
    "- `random_flip_left_right` : Flip the image\n",
    "- `random_brightness` : Randomly modify image brightness (use the `max_delta`, for example with a value of 32.0/255)\n",
    "- `random_saturation` : Randomly modify image brightness (use the `lower` and `upper` arguments, for example with values 0.5 and 1.5 respectively)\n",
    "\n",
    "Finally, in order to make sure the image values are still in the [0,1] interval, use the `clip_by_value` function.\n",
    "\n",
    "Remember to only apply these transformations to the train set, not the test set, as we want the test images to be the real ones."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-10T18:53:59.623028Z",
     "start_time": "2019-04-10T18:53:59.620102Z"
    }
   },
   "outputs": [],
   "source": [
    "def _image_augmentation(image, label):\n",
    "    # Flip image\n",
    "    image = tf.image.random_flip_left_right(image)\n",
    "    # Random Brightness\n",
    "    image = tf.image.random_brightness(image, max_delta=32.0 / 255.0)\n",
    "    # Random Saturation\n",
    "    image = tf.image.random_saturation(image, lower=0.5, upper=1.5)\n",
    "    # Clip values between 0 and 1\n",
    "    image = tf.clip_by_value(image, 0.0, 1.0)\n",
    "\n",
    "    return image, label"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will now use `tf.data` to create the input data pipeline. It will be composed of the following steps :\n",
    "- Create the Dataset. Since we already have in-memory data, we will use the `from_tensor_slices` function\n",
    "- `map` the dataset with the preprocessing function implemented above\n",
    "- `map` the result with the image augmentation function\n",
    "- Add the `shuffle`, `repeat`, `batch` and `prefect` steps to configure training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-10T18:54:00.267556Z",
     "start_time": "2019-04-10T18:54:00.038452Z"
    }
   },
   "outputs": [],
   "source": [
    "BATCH_SIZE = 32\n",
    "SHUFFLE_SIZE = 10000\n",
    "NUM_EPOCHS = 5\n",
    "\n",
    "# Create Dataset\n",
    "ds_train = tf.data.Dataset.from_tensor_slices((train_images, train_labels))\n",
    "# Map for input preprocessing\n",
    "ds_train = ds_train.map(_preprocess_inputs)\n",
    "# Map for image augmentation\n",
    "ds_train = ds_train.map(_image_augmentation)\n",
    "# Shuffle / Repeat / Batch / Prefetch\n",
    "ds_train = ds_train.shuffle(SHUFFLE_SIZE).repeat(NUM_EPOCHS).batch(BATCH_SIZE).prefetch(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The input data pipeline for the test set is quite similar, except there is no need to use the `repeat` and `prefetch` steps. Moreover, the image augmentation phase is not necessary for the test set, as we want the real images to be used to evaluate the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-10T18:54:01.268317Z",
     "start_time": "2019-04-10T18:54:01.218703Z"
    }
   },
   "outputs": [],
   "source": [
    "# Create Dataset\n",
    "ds_test = tf.data.Dataset.from_tensor_slices((test_images, test_labels))\n",
    "# Map\n",
    "ds_test = ds_test.map(_preprocess_inputs)\n",
    "# Shuffle / Repeat / Batch / Prefetch\n",
    "ds_test = ds_test.shuffle(SHUFFLE_SIZE).batch(BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Management"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-10T18:54:02.534651Z",
     "start_time": "2019-04-10T18:54:02.530775Z"
    }
   },
   "outputs": [],
   "source": [
    "conv_base_url = \"https://tfhub.dev/google/tf2-preview/mobilenet_v2/feature_vector/2\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "solution2": "hidden",
    "solution2_first": true
   },
   "source": [
    "Our Neural Network will now be composed of a convolutional based coming from a trained network called `MobileNetV2`, followed by the following layer : \n",
    "- `KerasLayer` : add the `conv_base_url`, the output_shape ([OUTPUT_SHAPE]) and set `trainable` to False\n",
    "- `Dense` Layer : 10 neurons, softmax activation\n",
    "\n",
    "> <div class=\"mark\">Create the model</div><i class=\"fa fa-lightbulb-o \"></i>\n",
    "\n",
    "Documentation : \n",
    "- https://www.tensorflow.org/hub/api_docs/python/hub/KerasLayer?hl=en\n",
    "- https://www.tensorflow.org/api_docs/python/tf/keras/layers/Dense"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-10T18:54:10.159237Z",
     "start_time": "2019-04-10T18:54:08.395967Z"
    },
    "solution2": "hidden"
   },
   "outputs": [],
   "source": [
    "OUTPUT_SHAPE = 1280\n",
    "INPUT_SHAPE = 224\n",
    "NUM_CLASSES = 10\n",
    "\n",
    "model = tf.keras.Sequential()\n",
    "model.add(hub.KerasLayer(conv_base_url, output_shape=[OUTPUT_SHAPE], trainable=False))\n",
    "model.add(tf.keras.layers.Dense(NUM_CLASSES, activation='softmax'))\n",
    "\n",
    "model.build([None, INPUT_SHAPE, INPUT_SHAPE, 3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-24T19:27:28.138274Z",
     "start_time": "2019-03-24T19:27:28.017642Z"
    }
   },
   "outputs": [],
   "source": [
    "OUTPUT_SHAPE = 1280\n",
    "INPUT_SHAPE = 224\n",
    "NUM_CLASSES = 10\n",
    "\n",
    "model = tf.keras.models.Sequential()\n",
    "# TODO\n",
    "\n",
    "model.build([None, INPUT_SHAPE, INPUT_SHAPE, 3])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To make our network ready for training, we need to pick three more things, as part of \"compilation\" step:\n",
    "\n",
    "* A loss function: the is how the network will be able to measure how good a job it is doing on its training data, and thus how it will be able to steer itself in the right direction.\n",
    "* An optimizer: this is the mechanism through which the network will update itself based on the data it sees and its loss function.\n",
    "* Metrics to monitor during training and testing. Here we will only care about accuracy (the fraction of the images that were correctly classified).\n",
    "\n",
    "You will implement the following compilation step for your Neural Network : \n",
    "- \"adam\" optimizer\n",
    "- \"categorical_crossentropy\" loss\n",
    "- metric : \"accuracy\"\n",
    "\n",
    "Documentation : https://www.tensorflow.org/api_docs/python/tf/keras/models/Sequential#compile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-10T18:54:12.537783Z",
     "start_time": "2019-04-10T18:54:12.535371Z"
    }
   },
   "outputs": [],
   "source": [
    "optimizer = tf.optimizers.Adam()\n",
    "\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer=optimizer,\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Summarize the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-10T18:54:13.699660Z",
     "start_time": "2019-04-10T18:54:13.691419Z"
    }
   },
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-19T10:14:34.658684Z",
     "start_time": "2019-03-19T10:14:34.653107Z"
    }
   },
   "source": [
    "We are now ready to train our network, which in Keras is done via a call to the `fit` method of the network: \n",
    "we \"fit\" the model to its training data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You will fit the network with the following configurations :\n",
    "- `x`: ds_train\n",
    "- `epochs` : 5 (passes on the whole dataset)\n",
    "- `steps_per_epoch`: 1000 steps\n",
    "- `validation_data`: ds_test\n",
    "- `validation_steps`: 10\n",
    "- `callbacks`: tensorboard\n",
    "\n",
    "You will also add a callback for launching TensorBoard to observe how the training is performing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-10T18:54:19.067422Z",
     "start_time": "2019-04-10T18:54:19.065049Z"
    }
   },
   "outputs": [],
   "source": [
    "LOG_DIR = './tensorboard/tf_keras_data_transfer'\n",
    "\n",
    "tensorboard = tf.keras.callbacks.TensorBoard(log_dir=LOG_DIR, histogram_freq=1, update_freq=\"batch\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "solution2": "hidden",
    "solution2_first": true
   },
   "source": [
    "> <div class=\"mark\">Fit the model with the above information.</div><i class=\"fa fa-lightbulb-o \"></i>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-10T19:10:00.989321Z",
     "start_time": "2019-04-10T18:54:23.242442Z"
    },
    "solution2": "hidden"
   },
   "outputs": [],
   "source": [
    "shutil.rmtree(LOG_DIR, ignore_errors=True)\n",
    "\n",
    "model.fit(x=ds_train,\n",
    "         epochs=NUM_EPOCHS,\n",
    "         steps_per_epoch=1000,\n",
    "         validation_data=ds_test,\n",
    "         validation_steps=10,\n",
    "         callbacks=[tensorboard])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shutil.rmtree(LOG_DIR, ignore_errors=True)\n",
    "\n",
    "model. # TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Two quantities are being displayed during training: the \"loss\" of the network over the training data, and the accuracy of the network over the training data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Performance Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "solution2": "hidden",
    "solution2_first": true
   },
   "source": [
    "Now let's check that our model performs well on the test set too.\n",
    "\n",
    "You can do this by calling the `evaluate` method of your network on the test set (use 300 for the `steps` argument).\n",
    "\n",
    "Documentation : https://www.tensorflow.org/api_docs/python/tf/keras/models/Sequential#evaluate\n",
    "\n",
    "> <div class=\"mark\">Evaluate the model performance on test set</div><i class=\"fa fa-lightbulb-o \"></i>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-01T07:23:45.634450Z",
     "start_time": "2019-04-01T07:23:45.244985Z"
    },
    "solution2": "hidden"
   },
   "outputs": [],
   "source": [
    "model.evaluate(ds_test, steps=300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model. # TODO"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {
    "height": "256px",
    "width": "264px"
   },
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "384px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
