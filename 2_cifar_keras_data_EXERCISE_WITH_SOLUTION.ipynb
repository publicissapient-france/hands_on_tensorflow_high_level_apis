{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cifar10 with tf.keras and tf.data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-09T09:56:31.374816Z",
     "start_time": "2019-04-09T09:56:30.438961Z"
    }
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "import shutil\n",
    "import tensorflow as tf\n",
    "\n",
    "import numpy as np\n",
    "%matplotlib inline\n",
    "import random\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-09T09:56:31.379918Z",
     "start_time": "2019-04-09T09:56:31.376155Z"
    }
   },
   "outputs": [],
   "source": [
    "# Python version 3.5 or 3.6\n",
    "assert sys.version_info >= (3, 5)\n",
    "assert sys.version_info < (3, 7)\n",
    "# Tensorflow 2.0\n",
    "assert tf.__version__ >= \"2.0\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The problem we are trying to solve here is to classify RGB images (32 pixels by 32 pixels), into their 10 categories (_airplane_, _automobile_, _bird_, _cat_, _deer_, _dog_, _frog_, _horse_, _ship_, _truck_). The dataset we will use is the CIFAR10 dataset, a classic dataset in the machine learning community."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Input Data Management"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Download the dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The CIFAR10 dataset comes pre-loaded in Keras, in the form of a set of four Numpy arrays.\n",
    "\n",
    "Documentation : https://www.tensorflow.org/api_docs/python/tf/keras/datasets/cifar10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-09T09:56:32.715533Z",
     "start_time": "2019-04-09T09:56:32.426343Z"
    }
   },
   "outputs": [],
   "source": [
    "(train_images, train_labels), (test_images, test_labels) = tf.keras.datasets.cifar10.load_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualize the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's print the shapes of the datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-09T09:56:34.114825Z",
     "start_time": "2019-04-09T09:56:34.108808Z"
    }
   },
   "outputs": [],
   "source": [
    "print(\"Train images shape : {}\".format(train_images.shape))\n",
    "print(\"Train labels shape : {}\".format(train_labels.shape))\n",
    "print(\"Test images shape : {}\".format(test_images.shape))\n",
    "print(\"Test labels shape : {}\".format(test_labels.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see how the images look. This function shows a random example along with it's corresponding label."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-09T09:56:35.429808Z",
     "start_time": "2019-04-09T09:56:35.274654Z"
    }
   },
   "outputs": [],
   "source": [
    "i = random.randint(0, 100)\n",
    "\n",
    "print(\"Label: %s\" % train_labels[i])\n",
    "plt.imshow(train_images[i], cmap='gray')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Thats a little blurry !"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our workflow will be as follow: first we will present our neural network with the training data, `train_images` and `train_labels`. The network will then learn to associate images and labels. Finally, we will ask the network to produce predictions for `test_images`, and we will verify if these predictions match the labels from `test_labels`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create a tf.data Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-20T15:16:57.555113Z",
     "start_time": "2019-03-20T15:16:57.552806Z"
    },
    "solution2": "hidden",
    "solution2_first": true
   },
   "source": [
    "We will implement the input preprocessing function, composed of the following steps :\n",
    "- `cast` image to float32\n",
    "- `divide` image values by 255\n",
    "- `one_hot` encoding of the label\n",
    "> <span class=\"mark\">Implement the input processing function</span>\n",
    "\n",
    "Documentation\n",
    "- https://www.tensorflow.org/api_docs/python/tf/dtypes/cast\n",
    "- https://www.tensorflow.org/api_docs/python/tf/math/divide\n",
    "- https://www.tensorflow.org/api_docs/python/tf/one_hot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-09T09:56:40.353113Z",
     "start_time": "2019-04-09T09:56:40.341445Z"
    },
    "solution2": "hidden"
   },
   "outputs": [],
   "source": [
    "NUM_CAT = 10\n",
    "\n",
    "def _preprocess_inputs(img, label):\n",
    "    # Cast image to float32\n",
    "    img_float = tf.cast(img, tf.float32)\n",
    "    # Divide image values by 255\n",
    "    img_norm = tf.divide(img_float, 255)\n",
    "    \n",
    "    label_int = label[0]\n",
    "    # One Hot encoding of the label\n",
    "    label_one_hot = tf.one_hot(label_int, depth=NUM_CAT)\n",
    "    \n",
    "    return img_norm, label_one_hot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_CAT = 10\n",
    "\n",
    "def _preprocess_inputs(img, label):\n",
    "    # Cast image to float32\n",
    "    img_float = # TODO\n",
    "    # Divide image values by 255\n",
    "    img_norm = # TODO\n",
    "    \n",
    "    label_int = label[0]\n",
    "    # One Hot encoding of the label\n",
    "    label_one_hot = # TODO\n",
    "    \n",
    "    return img_norm, label_one_hot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "solution2": "hidden",
    "solution2_first": true
   },
   "source": [
    "We will apply image augmentation in the input processing pipeline. Here are the transformations we can apply :\n",
    "- `random_flip_left_right` : Flip the image\n",
    "- `random_brightness` : Randomly modify image brightness (use the `max_delta`, for example with a value of 32.0/255)\n",
    "- `random_saturation` : Randomly modify image brightness (use the `lower` and `upper` arguments, for example with values 0.5 and 1.5 respectively)\n",
    "\n",
    "Finally, in order to make sure the image values are still in the [0,1] interval, use the `clip_by_value` function.\n",
    "\n",
    "Remember to only apply these transformations to the train set, not the test set, as we want the test images to be the real ones.\n",
    "\n",
    "> <div class=\"mark\">Create the _train_preprocess function to apply image augmentation.</div><i class=\"fa fa-lightbulb-o \"></i>\n",
    "\n",
    "Documentation : \n",
    "- https://www.tensorflow.org/api_docs/python/tf/image/random_flip_left_right\n",
    "- https://www.tensorflow.org/api_docs/python/tf/image/random_brightness\n",
    "- https://www.tensorflow.org/api_docs/python/tf/image/random_saturation\n",
    "- https://www.tensorflow.org/api_docs/python/tf/clip_by_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-09T09:56:44.664741Z",
     "start_time": "2019-04-09T09:56:44.658538Z"
    },
    "solution2": "hidden"
   },
   "outputs": [],
   "source": [
    "def _image_augmentation(image, label):\n",
    "    # Flip image\n",
    "    image = tf.image.random_flip_left_right(image)\n",
    "    # Random Brightness\n",
    "    image = tf.image.random_brightness(image, max_delta=32.0 / 255.0)\n",
    "    # Random Saturation\n",
    "    image = tf.image.random_saturation(image, lower=0.5, upper=1.5)\n",
    "    # Clip values between 0 and 1\n",
    "    image = tf.clip_by_value(image, 0.0, 1.0)\n",
    "\n",
    "    return image, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _image_augmentation(image, label):\n",
    "    # Flip image\n",
    "    image = # TODO\n",
    "    # Random Brightness\n",
    "    image = # TODO\n",
    "    # Random Saturation\n",
    "    image = # TODO\n",
    "    # Clip values between 0 and 1\n",
    "    image = # TODO\n",
    "\n",
    "    return image, label"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "solution2": "hidden",
    "solution2_first": true
   },
   "source": [
    "We will now use `tf.data` to create the input data pipeline. It will be composed of the following steps :\n",
    "- Create the Dataset. Since we already have in-memory data, we will use the `from_tensor_slices` function\n",
    "- `map` the dataset with the preprocessing function implemented above\n",
    "- `map` the result with the image augmentation function\n",
    "- Add the `shuffle`, `repeat`, `batch` and `prefect` steps to configure training\n",
    "\n",
    "> <span class=\"mark\">Implement the input data pipeline with the above descriptions for the training set</span>\n",
    "\n",
    "Documentation ! \n",
    "- https://www.tensorflow.org/api_docs/python/tf/data/Dataset#from_tensor_slices\n",
    "- https://www.tensorflow.org/api_docs/python/tf/data/Dataset#map\n",
    "- https://www.tensorflow.org/api_docs/python/tf/data/Dataset#shuffle\n",
    "- https://www.tensorflow.org/api_docs/python/tf/data/Dataset#repeat\n",
    "- https://www.tensorflow.org/api_docs/python/tf/data/Dataset#batch\n",
    "- https://www.tensorflow.org/api_docs/python/tf/data/Dataset#prefetch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-09T09:56:49.247903Z",
     "start_time": "2019-04-09T09:56:48.865225Z"
    },
    "solution2": "hidden"
   },
   "outputs": [],
   "source": [
    "BATCH_SIZE = 32\n",
    "SHUFFLE_SIZE = 10000\n",
    "NUM_EPOCHS = 5\n",
    "\n",
    "# Create Dataset\n",
    "ds_train = tf.data.Dataset.from_tensor_slices((train_images, train_labels))\n",
    "# Map for input preprocessing\n",
    "ds_train = ds_train.map(_preprocess_inputs)\n",
    "# Map for image augmentation\n",
    "ds_train = ds_train.map(_image_augmentation)\n",
    "# Shuffle / Repeat / Batch / Prefetch\n",
    "ds_train = ds_train.shuffle(SHUFFLE_SIZE).repeat(NUM_EPOCHS).batch(BATCH_SIZE).prefetch(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 32\n",
    "SHUFFLE_SIZE = 10000\n",
    "NUM_EPOCHS = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Dataset\n",
    "ds_train = tf.data.Dataset. # TODO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Map for input preprocessing\n",
    "ds_train = ds_train. # TODO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Map for image augmentation\n",
    "ds_train = # TODO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Shuffle / Repeat / Batch / Prefetch\n",
    "ds_train = ds_train. # TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "solution2": "hidden",
    "solution2_first": true
   },
   "source": [
    "The input data pipeline for the test set is quite similar, except there is no need to use the `repeat` and `prefetch` steps. Moreover, the image augmentation phase is not necessary for the test set, as we want the real images to be used to evaluate the results.\n",
    "\n",
    "> <span class=\"mark\">Implement the input data pipeline for the test set</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-09T09:56:53.391744Z",
     "start_time": "2019-04-09T09:56:53.302778Z"
    },
    "solution2": "hidden"
   },
   "outputs": [],
   "source": [
    "# Create Dataset\n",
    "ds_test = tf.data.Dataset.from_tensor_slices((test_images, test_labels))\n",
    "# Map\n",
    "ds_test = ds_test.map(_preprocess_inputs)\n",
    "# Shuffle / Repeat / Batch / Prefetch\n",
    "ds_test = ds_test.shuffle(SHUFFLE_SIZE).batch(BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Dataset\n",
    "ds_test = tf.data.Dataset. # TODO\n",
    "# Map for input preprocessing\n",
    "ds_test = # TODO\n",
    "# Shuffle / Repeat / Batch / Prefetch\n",
    "ds_test = # TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's check the shapes of the generated tensors :\n",
    "- Image batch shape must be (32, 32, 32, 3)\n",
    "- Labels batch shape must be (32, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-09T09:56:57.293995Z",
     "start_time": "2019-04-09T09:56:56.253966Z"
    }
   },
   "outputs": [],
   "source": [
    "for item in ds_train:\n",
    "    print(\"Image Batch shape : {}\".format(item[0].shape))\n",
    "    print(\"Labels Batch shape: {}\".format(item[1].shape))\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Management"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-20T14:11:19.058875Z",
     "start_time": "2019-03-20T14:11:19.056718Z"
    }
   },
   "source": [
    "## Build the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our Neural Network will contain the following building blocks : \n",
    "- Conv2D Layer : 16 filters, (3, 3) kernel, relu activation, input shape (32, 32, 3)\n",
    "- MaxPooling2D : pool size (2, 2)\n",
    "- Conv2D Layer : 32 filters, (3, 3) kernel, relu activation\n",
    "- MaxPooling2D : pool size (2, 2)\n",
    "- Flatten layer\n",
    "- Dense Layer : 10 neurons, softmax activation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-09T09:56:59.674934Z",
     "start_time": "2019-04-09T09:56:59.605800Z"
    }
   },
   "outputs": [],
   "source": [
    "model = tf.keras.models.Sequential()\n",
    "model.add(tf.keras.layers.Conv2D(filters=16, kernel_size=(3, 3), activation='relu', input_shape=(32, 32, 3)))\n",
    "model.add(tf.keras.layers.MaxPooling2D((2, 2)))\n",
    "model.add(tf.keras.layers.Conv2D(filters=32, kernel_size=(3, 3), activation='relu'))\n",
    "model.add(tf.keras.layers.MaxPooling2D((2, 2)))\n",
    "model.add(tf.keras.layers.Flatten())\n",
    "model.add(tf.keras.layers.Dense(10, activation='softmax'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To make our network ready for training, we need to pick three more things, as part of \"compilation\" step:\n",
    "\n",
    "* A loss function: the is how the network will be able to measure how good a job it is doing on its training data, and thus how it will be able to steer itself in the right direction.\n",
    "* An optimizer: this is the mechanism through which the network will update itself based on the data it sees and its loss function.\n",
    "* Metrics to monitor during training and testing. Here we will only care about accuracy (the fraction of the images that were correctly classified).\n",
    "\n",
    "You will implement the following compilation step for your Neural Network : \n",
    "- \"adam\" optimizer\n",
    "- \"categorical_crossentropy\" loss\n",
    "- metric : \"accuracy\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-09T09:57:01.991451Z",
     "start_time": "2019-04-09T09:57:01.894447Z"
    }
   },
   "outputs": [],
   "source": [
    "optimizer = tf.optimizers.Adam()\n",
    "\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer=optimizer,\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Summarize the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-09T09:57:03.400692Z",
     "start_time": "2019-04-09T09:57:03.387539Z"
    }
   },
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-20T14:18:08.113172Z",
     "start_time": "2019-03-20T14:18:08.109910Z"
    }
   },
   "source": [
    "We are now ready to train our network, which in Keras is done via a call to the `fit` method of the network: \n",
    "we \"fit\" the model to its training data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You will fit the network with the following configurations :\n",
    "- `x`: ds_train\n",
    "- `epochs` : 5 (passes on the whole dataset)\n",
    "- `steps_per_epoch`: 1000 steps\n",
    "- `validation_data`: ds_test\n",
    "- `validation_steps`: 10\n",
    "- `callbacks`: tensorboard\n",
    "\n",
    "Documentation : https://www.tensorflow.org/api_docs/python/tf/keras/models/Sequential#fit\n",
    "\n",
    "You will also add a callback for launching TensorBoard to observe how the training is performing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-09T09:57:06.926998Z",
     "start_time": "2019-04-09T09:57:06.918968Z"
    }
   },
   "outputs": [],
   "source": [
    "LOG_DIR = './tensorboard/tf_keras_data'\n",
    "\n",
    "tensorboard = tf.keras.callbacks.TensorBoard(log_dir=LOG_DIR, histogram_freq=1, update_freq=\"batch\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "solution2": "hidden",
    "solution2_first": true
   },
   "source": [
    "> <div class=\"mark\">Fit the model with the above information.</div><i class=\"fa fa-lightbulb-o \"></i>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-09T09:57:42.368968Z",
     "start_time": "2019-04-09T09:57:09.385403Z"
    },
    "solution2": "hidden"
   },
   "outputs": [],
   "source": [
    "shutil.rmtree(LOG_DIR, ignore_errors=True)\n",
    "\n",
    "model.fit(x=ds_train,\n",
    "         epochs=NUM_EPOCHS,\n",
    "         steps_per_epoch=1000,\n",
    "         validation_data=ds_test,\n",
    "         validation_steps=10,\n",
    "         callbacks=[tensorboard])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-20T14:38:36.234049Z",
     "start_time": "2019-03-20T14:37:52.263731Z"
    }
   },
   "outputs": [],
   "source": [
    "shutil.rmtree(LOG_DIR, ignore_errors=True)\n",
    "\n",
    "model. # TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Performance Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "solution2": "hidden",
    "solution2_first": true
   },
   "source": [
    "Now let's check that our model performs well on the test set too.\n",
    "\n",
    "You can do this by calling the `evaluate` method of your network on the test set (use 300 for the `steps` argument).\n",
    "\n",
    "Documentation : https://www.tensorflow.org/api_docs/python/tf/keras/models/Sequential#evaluate\n",
    "\n",
    "> <div class=\"mark\">Evaluate the model performance on test set</div><i class=\"fa fa-lightbulb-o \"></i>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-09T09:57:43.312157Z",
     "start_time": "2019-04-09T09:57:42.370443Z"
    },
    "solution2": "hidden"
   },
   "outputs": [],
   "source": [
    "model.evaluate(ds_test, steps=300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model. # TODO"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
